{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b5c396-a707-4630-b1f7-d0b0f7df964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "          ID  PRG   PL  PR  SK   TS   M11    BD2  Age  Insurance   Sepssis\n",
      "0  ICU200010    6  148  72  35    0  33.6  0.627   50          0  Positive\n",
      "1  ICU200011    1   85  66  29    0  26.6  0.351   31          0  Negative\n",
      "2  ICU200012    8  183  64   0    0  23.3  0.672   32          1  Positive\n",
      "3  ICU200013    1   89  66  23   94  28.1  0.167   21          1  Negative\n",
      "4  ICU200014    0  137  40  35  168  43.1  2.288   33          1  Positive\n",
      "\n",
      "Last 5 rows:\n",
      "            ID  PRG   PL  PR  SK   TS   M11    BD2  Age  Insurance   Sepssis\n",
      "594  ICU200604    6  123  72  45  230  33.6  0.733   34          0  Negative\n",
      "595  ICU200605    0  188  82  14  185  32.0  0.682   22          1  Positive\n",
      "596  ICU200606    0   67  76   0    0  45.3  0.194   46          1  Negative\n",
      "597  ICU200607    1   89  24  19   25  27.8  0.559   21          0  Negative\n",
      "598  ICU200608    1  173  74   0    0  36.8  0.088   38          1  Positive\n",
      "\n",
      "Random sample of 5 rows:\n",
      "            ID  PRG   PL  PR  SK  TS   M11    BD2  Age  Insurance   Sepssis\n",
      "94   ICU200104    2  142  82  18  64  24.7  0.761   21          1  Negative\n",
      "518  ICU200528   13   76  60   0   0  32.8  0.180   41          1  Negative\n",
      "238  ICU200248    9  164  84  21   0  30.8  0.831   32          0  Positive\n",
      "323  ICU200333   13  152  90  33  29  26.8  0.731   43          1  Positive\n",
      "5    ICU200015    5  116  74   0   0  25.6  0.201   30          1  Negative\n",
      "\n",
      "Data Shape: (599, 11)\n",
      "\n",
      "Data Types:\n",
      "ID            object\n",
      "PRG            int64\n",
      "PL             int64\n",
      "PR             int64\n",
      "SK             int64\n",
      "TS             int64\n",
      "M11          float64\n",
      "BD2          float64\n",
      "Age            int64\n",
      "Insurance      int64\n",
      "Sepssis       object\n",
      "dtype: object\n",
      "\n",
      "Summary Statistics for Numerical Features:\n",
      "              PRG          PL          PR          SK          TS         M11  \\\n",
      "count  599.000000  599.000000  599.000000  599.000000  599.000000  599.000000   \n",
      "mean     3.824708  120.153589   68.732888   20.562604   79.460768   31.920033   \n",
      "std      3.362839   32.682364   19.335675   16.017622  116.576176    8.008227   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   64.000000    0.000000    0.000000   27.100000   \n",
      "50%      3.000000  116.000000   70.000000   23.000000   36.000000   32.000000   \n",
      "75%      6.000000  140.000000   80.000000   32.000000  123.500000   36.550000   \n",
      "max     17.000000  198.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "              BD2         Age   Insurance  \n",
      "count  599.000000  599.000000  599.000000  \n",
      "mean     0.481187   33.290484    0.686144  \n",
      "std      0.337552   11.828446    0.464447  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.248000   24.000000    0.000000  \n",
      "50%      0.383000   29.000000    1.000000  \n",
      "75%      0.647000   40.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n",
      "\n",
      "Summary for Object/Categorical Features:\n",
      "               ID   Sepssis\n",
      "count         599       599\n",
      "unique        599         2\n",
      "top     ICU200010  Negative\n",
      "freq            1       391\n",
      "\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "All EDA plots have been saved into eda_results.pdf\n",
      "\n",
      "Hypothesis Testing Results:\n",
      "  Feature            Test     Statistic       p-value\n",
      "0     PRG  Mann-Whitney U  31693.000000  7.517335e-06\n",
      "1      PL  Mann-Whitney U  17812.500000  9.039444e-30\n",
      "2      PR  Mann-Whitney U  33654.500000  5.014050e-04\n",
      "3      SK          t-test     -1.852114  6.450285e-02\n",
      "4      TS  Mann-Whitney U  36708.500000  3.737172e-02\n",
      "5     M11  Mann-Whitney U  24464.000000  9.465745e-16\n",
      "6     BD2  Mann-Whitney U  31589.500000  6.800959e-06\n",
      "7     Age  Mann-Whitney U  26491.500000  1.971962e-12\n",
      "\n",
      "Scaled Feature Columns:\n",
      "Index(['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age', 'Insurance'], dtype='object')\n",
      "\n",
      "--- Basic Models Evaluation ---\n",
      "\n",
      "Logistic Regression Accuracy: 0.7417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.81        77\n",
      "           1       0.67      0.56      0.61        43\n",
      "\n",
      "    accuracy                           0.74       120\n",
      "   macro avg       0.72      0.70      0.71       120\n",
      "weighted avg       0.74      0.74      0.74       120\n",
      "\n",
      "\n",
      "Random Forest Accuracy: 0.7083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77        77\n",
      "           1       0.60      0.58      0.59        43\n",
      "\n",
      "    accuracy                           0.71       120\n",
      "   macro avg       0.68      0.68      0.68       120\n",
      "weighted avg       0.71      0.71      0.71       120\n",
      "\n",
      "\n",
      "SVM Accuracy: 0.7583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82        77\n",
      "           1       0.68      0.63      0.65        43\n",
      "\n",
      "    accuracy                           0.76       120\n",
      "   macro avg       0.74      0.73      0.73       120\n",
      "weighted avg       0.76      0.76      0.76       120\n",
      "\n",
      "\n",
      "--- Stacking Classifier Evaluation ---\n",
      "Stacking Model Accuracy: 0.7333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        77\n",
      "           1       0.63      0.63      0.63        43\n",
      "\n",
      "    accuracy                           0.73       120\n",
      "   macro avg       0.71      0.71      0.71       120\n",
      "weighted avg       0.73      0.73      0.73       120\n",
      "\n",
      "\n",
      "Gradient Boosting Accuracy: 0.7666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82        77\n",
      "           1       0.67      0.70      0.68        43\n",
      "\n",
      "    accuracy                           0.77       120\n",
      "   macro avg       0.75      0.75      0.75       120\n",
      "weighted avg       0.77      0.77      0.77       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakash\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:42:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Accuracy: 0.7333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79        77\n",
      "           1       0.62      0.67      0.64        43\n",
      "\n",
      "    accuracy                           0.73       120\n",
      "   macro avg       0.71      0.72      0.72       120\n",
      "weighted avg       0.74      0.73      0.74       120\n",
      "\n",
      "\n",
      "KNN Accuracy: 0.7583333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82        77\n",
      "           1       0.69      0.58      0.63        43\n",
      "\n",
      "    accuracy                           0.76       120\n",
      "   macro avg       0.74      0.72      0.73       120\n",
      "weighted avg       0.75      0.76      0.75       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakash\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGM Accuracy: 0.7333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79        77\n",
      "           1       0.62      0.65      0.64        43\n",
      "\n",
      "    accuracy                           0.73       120\n",
      "   macro avg       0.71      0.72      0.71       120\n",
      "weighted avg       0.74      0.73      0.73       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakash\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Prakash\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bi-SGM Accuracy: 0.7583333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82        77\n",
      "           1       0.69      0.58      0.63        43\n",
      "\n",
      "    accuracy                           0.76       120\n",
      "   macro avg       0.74      0.72      0.73       120\n",
      "weighted avg       0.75      0.76      0.75       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Importing Libraries\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Bidirectional\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# =========================\n",
    "# Load Dataset\n",
    "# =========================\n",
    "# Adjust the file path if needed\n",
    "df = pd.read_csv('Paitients_Files_Train.csv')\n",
    "\n",
    "# Display DataFrame information\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df.tail())\n",
    "print(\"\\nRandom sample of 5 rows:\")\n",
    "print(df.sample(5))\n",
    "print(\"\\nData Shape:\", df.shape)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nSummary Statistics for Numerical Features:\")\n",
    "print(df.describe())\n",
    "print(\"\\nSummary for Object/Categorical Features:\")\n",
    "print(df.describe(include=['object']))\n",
    "\n",
    "# =========================\n",
    "# Handling Missing Values (Optional)\n",
    "# =========================\n",
    "# If certain columns have zeros that represent missing values, uncomment the block below and adjust columns as needed.\n",
    "# cols_with_invalid_zeros = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2']\n",
    "# for col in cols_with_invalid_zeros:\n",
    "#     df.loc[df[col] == 0, col] = np.nan\n",
    "# print(\"Missing values per column after replacing zeros with NaN:\")\n",
    "# print(df.isnull().sum())\n",
    "\n",
    "# Optional median imputation for selected columns:\n",
    "# cols_to_impute = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11']\n",
    "# imputer = SimpleImputer(strategy='median')\n",
    "# df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "# print(\"Missing values after imputation:\")\n",
    "# print(df[cols_to_impute].isnull().sum())\n",
    "# print(\"\\nUpdated summary statistics for imputed columns:\")\n",
    "# print(df[cols_to_impute].describe())\n",
    "\n",
    "# =========================\n",
    "# Checking For Duplicates\n",
    "# =========================\n",
    "duplicates = df.duplicated().sum()\n",
    "print(\"\\nNumber of duplicate rows:\", duplicates)\n",
    "if duplicates > 0:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(\"Duplicates removed.\")\n",
    "\n",
    "# =========================\n",
    "# Exploratory Data Analysis (EDA)\n",
    "# =========================\n",
    "# Define feature and target columns.\n",
    "# Change 'Sepssis' to 'Sepsis' if that is the correct column name.\n",
    "features = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age']\n",
    "target = 'Sepssis'\n",
    "# Ensure the target is categorical\n",
    "df[target] = df[target].astype('category')\n",
    "\n",
    "# Define PDF filename for saving plots\n",
    "pdf_filename = 'eda_results.pdf'\n",
    "\n",
    "# Create PDF with boxplots, histograms, and correlation heatmap.\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    # 1. Boxplots grouped by target\n",
    "    for feature in features:\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        sns.boxplot(x=target, y=feature, data=df, ax=ax)\n",
    "        ax.set_title(f'Boxplot of {feature} by Sepsis Outcome')\n",
    "        ax.set_xlabel('Sepsis')\n",
    "        ax.set_ylabel(feature)\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # 2. Histograms with KDE\n",
    "    for feature in features:\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        sns.histplot(df[feature], kde=True, bins=30, ax=ax)\n",
    "        ax.set_title(f'Histogram of {feature}')\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # 3. Correlation Heatmap\n",
    "    corr_matrix = df[features].corr()\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", ax=ax)\n",
    "    ax.set_title('Correlation Matrix of Features')\n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"\\nAll EDA plots have been saved into {pdf_filename}\")\n",
    "\n",
    "# =========================\n",
    "# Hypothesis Testing\n",
    "# =========================\n",
    "# Convert target to binary (mapping: Negative -> 0, Positive -> 1)\n",
    "df[target] = df[target].map({'Negative': 0, 'Positive': 1})\n",
    "\n",
    "# Function to choose test based on skewness\n",
    "def choose_test(series):\n",
    "    return abs(series.skew()) < 0.5  # True for approximate normality\n",
    "\n",
    "results = []\n",
    "for feature in features:\n",
    "    group0 = df[df[target] == 0][feature]\n",
    "    group1 = df[df[target] == 1][feature]\n",
    "    \n",
    "    if choose_test(group0) and choose_test(group1):\n",
    "        test_stat, p_value = ttest_ind(group0, group1, nan_policy='omit')\n",
    "        test_used = 't-test'\n",
    "    else:\n",
    "        test_stat, p_value = mannwhitneyu(group0, group1, alternative='two-sided')\n",
    "        test_used = 'Mann-Whitney U'\n",
    "    \n",
    "    results.append({\n",
    "        'Feature': feature,\n",
    "        'Test': test_used,\n",
    "        'Statistic': test_stat,\n",
    "        'p-value': p_value\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nHypothesis Testing Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# =========================\n",
    "# Feature Scaling\n",
    "# =========================\n",
    "# Apply Power Transformation to selected features to reduce skewness.\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "for col in [\"SK\", \"TS\", \"M11\", \"BD2\", \"Age\"]:\n",
    "    df[col] = transformer.fit_transform(df[[col]])\n",
    "\n",
    "# Remove unnecessary column if present.\n",
    "if \"ID\" in df.columns:\n",
    "    df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Separate features and target.\n",
    "X = df.drop(columns=[target])\n",
    "Y = df[target]\n",
    "\n",
    "# Apply Robust Scaling to all features.\n",
    "scaler = RobustScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "print(\"\\nScaled Feature Columns:\")\n",
    "print(df_scaled.columns)\n",
    "\n",
    "# =========================\n",
    "# Train-Test Split for Modeling\n",
    "# =========================\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =========================\n",
    "# Model Training and Evaluation\n",
    "# =========================\n",
    "\n",
    "# --- 1. Basic Models ---\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC()\n",
    "}\n",
    "\n",
    "print(\"\\n--- Basic Models Evaluation ---\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- 2. Stacking Classifier ---\n",
    "base_models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('SVM', SVC(probability=True))\n",
    "]\n",
    "meta_model = LogisticRegression()\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "print(\"\\n--- Stacking Classifier Evaluation ---\")\n",
    "print(f\"Stacking Model Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "print(classification_report(y_test, y_pred_stack))\n",
    "\n",
    "# --- 3. Additional Models ---\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "print(\"\\nGradient Boosting Accuracy:\", accuracy_score(y_test, gb_pred))\n",
    "print(classification_report(y_test, gb_pred))\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "print(\"\\nXGBoost Accuracy:\", accuracy_score(y_test, xgb_pred))\n",
    "print(classification_report(y_test, xgb_pred))\n",
    "\n",
    "# KNN\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "print(\"\\nKNN Accuracy:\", accuracy_score(y_test, knn_pred))\n",
    "print(classification_report(y_test, knn_pred))\n",
    "\n",
    "# SGM (Simple Neural Network)\n",
    "sgm_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)\n",
    "sgm_model.fit(X_train, y_train)\n",
    "sgm_pred = sgm_model.predict(X_test)\n",
    "print(\"\\nSGM Accuracy:\", accuracy_score(y_test, sgm_pred))\n",
    "print(classification_report(y_test, sgm_pred))\n",
    "\n",
    "# Bi-SGM (Deeper Neural Network)\n",
    "bi_sgm_model = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=500)\n",
    "bi_sgm_model.fit(X_train, y_train)\n",
    "bi_sgm_pred = bi_sgm_model.predict(X_test)\n",
    "print(\"\\nBi-SGM Accuracy:\", accuracy_score(y_test, bi_sgm_pred))\n",
    "print(classification_report(y_test, bi_sgm_pred))\n",
    "\n",
    "# --- 4. RNN and Bi-RNN Models ---\n",
    "# For RNN-based models, we need to reshape the input data.\n",
    "X_train_rnn = np.reshape(X_train.values, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_rnn = np.reshape(X_test.values, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# RNN Model\n",
    "rnn_model = Sequential([\n",
    "    SimpleRNN(50, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
